---
title: "Case Study C - HIV"
author: "Oumaima Al Qoh, Francisco Arrieta, Lucia Camenisch, Manuela Giansante, Emily Schmidt, Camille Beatrice Valera"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: true # creating a table of contents (toc)
    toc_float: 
      collapsed: false # toc does not collapse and is shown as a sidebar (toc_float)
    number_sections: true # document sections are numbered
    theme: cosmo
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r packages}
library(data.table)
library(ggplot2)
library(e1071)
library(missForest)
library(caret)
library(MASS)
library(gridExtra)
library(fabletools)
library(forecast)
library(feasts)
```


```{r}
# Confusion matrix
draw_confusion_matrix <- function(cm, titleaddon = '') {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title(paste0('CONFUSION MATRIX', ' ', titleaddon), cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='pink')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='orange')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='orange')
  rect(250, 305, 340, 365, col='pink')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(5, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(5, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(23, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(23, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(41, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(41, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(59, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(59, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(77, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(77, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  text(95, 85, names(cm$byClass[8]), cex=1.2, font=2)
  text(95, 70, round(as.numeric(cm$byClass[8]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```





```{r}
aids <- fread("Caids.csv")

aids[, sex := as.factor(sex)]
aids[, type := as.factor(type)]

new_DF <- aids[rowSums(is.na(aids)) > 0,]
```


```{r}
par(mfrow = c(3,2))

hist(aids$cd4)
hist(sqrt(aids$cd4))
hist(aids$cd8)
hist(sqrt(aids$cd8))
hist(aids$rna)
hist(log(aids$rna))
```

```{r}
aids_melt <- melt(aids[, -1], id.vars = "type")
ggplot(data = aids_melt[type %in% "DP",], aes(value)) + geom_histogram() +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "DP")
ggplot(data = aids_melt[type %in% "CP",], aes(value)) + geom_histogram() +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "CP")
```


```{r}
cor(aids[, 3:5], use = "complete.obs")
```


```{r}
skewness(sqrt(aids$cd4), na.rm = TRUE)
skewness(sqrt(aids$cd8), na.rm = TRUE)
skewness(log(aids$rna), na.rm = TRUE)
```

```{r}
aids[, sex := ifelse(sex == "m", 0, 1)]
aids[, type := ifelse(type == "DP", 0, 1)]

plot(aids[, 3:5], col = aids$type)

summary(aids)
```


```{r}
aids_completed <- missForest(aids, variablewise = TRUE)

summary(aids)
summary(aids_completed$ximp)
```

```{r}
aids <- aids_completed$ximp
```


# pre-processing: BoxCox

```{r}

lambda1<-BoxCox.lambda(aids$cd4)
lambda2<-BoxCox.lambda(aids$cd8)
lambda3<-BoxCox.lambda(aids$rna)

aids$cd4 <-  BoxCox(aids$cd4, lambda1)
aids$cd8 <-  BoxCox(aids$cd8, lambda2)
aids$rna <-  BoxCox(aids$rna, lambda3)

hist(aids$cd4)
hist(aids$cd8)
hist(aids$rna)

plot(aids)

```

# emily's training and  test set
```{r}
# Partitioning into training (60%) and validation (40%) 
train.index <- sample(c(1:dim(aids)[1]), dim(aids)[1]*0.6)
test.index <- setdiff(c(1:dim(aids)[1]), train.index)  

# Collect all the columns with training rows into training set 
train <-aids[train.index, ]
test <- aids[test.index, ]
```

# LDA (normal distributions)

 LDA works by finding a linear combination of the predictors that maximizes the separation between the classes, while minimizing the variation within each class. 

 The method assumes that the data can be separated into classes based on linear combination of the input features.
If there is not separation in the data logit regression will out-perform LDA.

Coefficients of linear discriminants: These display the linear combination of predictor variables that are used to form the decision rule of the LDA model. 
However, unlike tree-based models, LDA assumes that the predictors are normally distributed and that the covariance matrices are equal for all classes, so the variable importance measures may not be appropriate for all types of data.

Equal covariance matrices assumption:
```{r}
plot <- list()
box_variables <- c("sex", "cd4", "cd8", "rna")
for(i in box_variables) {
  plot[[i]] <- ggplot(train, aes(x = factor(type), y = get(i), fill = factor(type))) +
    geom_boxplot(alpha = 0.2) + 
    theme(legend.position = "none") + 
    scale_fill_manual(values = c("blue", "red")) +
    labs(x = "Type", y = i)
}

# Combine the boxplots into a single plot
do.call(grid.arrange, c(plot, nrow = 1))
```

We fulfil the covariance matrix assumption.


```{r}
set.seed(1)

# Fit the model
DA1 <- lda(type ~., data= train)
# the response variable is the grouping factor 
DA1
plot(DA1)
importance<- as.data.frame(melt(DA1$scaling^2))
ggplot(importance) + geom_col(aes(x=Var1 ,y= value , fill= value), showlegend = F)+ theme_classic()

```

                    Actual Positive     Actual Negative
Predicted Positive   True Positive   False Positive
Predicted Negative   False Negative  True Negative

The four entries in the confusion matrix correspond to:

True Positive (TP): the number of instances that are correctly classified as positive (i.e., the classifier correctly identifies the positive cases).
False Positive (FP): the number of instances that are incorrectly classified as positive (i.e., the classifier identifies negative cases as positive).
False Negative (FN): the number of instances that are incorrectly classified as negative (i.e., the classifier identifies positive cases as negative).
True Negative (TN): the number of instances that are correctly classified as negative (i.e., the classifier correctly identifies the negative cases).
From these four entries, we can compute various performance metrics that are commonly used to evaluate the performance of a classifier. Two of the most commonly used metrics are sensitivity and specificity:

Sensitivity (also known as recall or true positive rate) is the proportion of true positives among all actual positives:

Sensitivity = TP / (TP + FN)

In other words, sensitivity measures the ability of the classifier to correctly identify the positive cases.

Specificity (also known as true negative rate) is the proportion of true negatives among all actual negatives:

Specificity = TN / (TN + FP)

In other words, specificity measures the ability of the classifier to correctly identify the negative cases.

Both sensitivity and specificity are important metrics for evaluating the performance of a classifier. In general, a good classifier should have high values of both sensitivity and specificity, although the relative importance of these metrics may depend on the specific application. For example, in a medical diagnostic test, high sensitivity is typically more important than high specificity, since a false negative result (i.e., a positive case incorrectly classified as negative) can have serious consequences.


```{r}
predictions<- predict(DA1, test)
DA1_test_prob<- predictions$posterior[,2]
DA1_test_resp<-as_factor(predictions$class)
test$type<-as_factor(test$type)
fourfoldplot(confusionMatrix(DA1_test_resp, as.factor(test$type))$table)
confusionMatrix(DA1_test_resp, as.factor(test$type))
# positive class 0!!!!!
# can be switched
```

Accuracy : 0.7143
Sensitivity : 0.9024
Specificity : 0.2000


#QDA

QDA works by finding a quadratic function of the predictors that maximizes the separation between the classes, while minimizing the variation within each class.

- Observation of each class is drawn from a normal distribution (same as LDA).
- QDA assumes that each class has its own covariance matrix
```{r}
set.seed(1)

# Fit the model
DA2 <- qda(type ~., data= train)
# the response variable is the grouping factor 
DA2

importance2<- as.data.frame(melt(DA2$scaling^2))
ggplot(importance2) + geom_col(aes(x=Var1 ,y= value , fill= Var1), showlegend = F)+ theme_classic()

```

```{r}

predictions2<- predict(DA2, test)
DA2_test_prob<- predictions$posterior[,2]
DA2_test_resp<-as_factor(predictions2$class)
fourfoldplot(confusionMatrix(DA2_test_resp, as.factor(test$type))$table)
confusionMatrix(DA2_test_resp, as.factor(test$type))
```
 Accuracy 0.70
Sensitivity : 0.8780 
Specificity : 0.2333

# tree, bagging, boosting, random forests give importance

# logistic regression importance through coeffs



